---
output:
  pdf_document: default
  html_document: default
---

# Using a testing set to evaluate the quality of a model

In this lab activity, we will do the following:
- load a dataset
- randomly divide the dataset into training and testing data
- train a linear model and a regression tree using the training set
- compute training error
- compute testing error

## Dependencies and setup

We'll use the following packages in this lab activity:

```{r}
library(tidyverse)
library(modelr)
library(rpart)      # Used to build a regression tree
library(rpart.plot) # Used to visualize trees
library(caret)
```

You'll need to install any packages that you don't already have installed.

## Loading and preprocessing the data

In this lab activity, we'll be using the Seoul bike dataset from the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand).
This dataset contains the number of public bikes rented at each hour in a Seoul bike sharing system.
In addition to the number of bikes, the data contains other variables, including weather and holiday information.
Our goal will be to build some simple models that predict the number of bikes rented as a function of other attributes available in the dataset.

```{r}
# Load data from file (you will need to adjust the file path to run locally)
data <- read_csv("gvsu-cis331-2023w/lecture-material/week-08/SeoulBikeData.csv")
```

We can look at the attributes of our data:

```{r}
colnames(data)
```

Notice that for each hour, we have information that we might expect to influence bike rentals, such as the weather, time of day, season, etc.
`rented_bike_count` gives the number of bikes rented for a particular hour.
Our goal in this lab will be to train a decision tree classifier to predict whether bike rentals will be high, medium, or low depending on the weather/time of day.

Notice that the bike rental count is numeric instead of symbolic, and we just want to predict low/medium/high rentals instead of the exact number of rentals.
So, let's go ahead and discretize the bike rental count into low/medium/high categories based on some thresholds:

```{r}
low_threshold <- 150
high_threshold <- 1000
categorize_rental_level <- function(rentals) {
  if (rentals <= low_threshold) {
    return("low")
  } else if (rentals >= high_threshold) {
    return("high")
  } else {
    return("medium")
  }
}
data$rental_level <- mapply(
  categorize_rental_level,
  data$rented_bike_count
)
```

Next, we want to convert categorical attributes into proper factors.

```{r}
# Convert categorical variables into factors:
data$holiday <- as.factor(data$holiday)
data$functioning_day <- as.factor(data$functioning_day)
data$seasons <- as.factor(data$seasons)
data$rental_level <- as.factor(data$rental_level)
```

## Creating training and testing sets

Next, we want to split our full data set into a training set and a testing set.
We'll use the training set to train/build our models, and then we can use the testing set to evaluate the performance of our model on data unseen during training.

```{r}
# First assign ID to each row to help us make the split.
data <- data %>%
  mutate(id = row_number())

# Size (as a proportion) of our training set.
training_set_size <- 0.5
```

**Challenge problem:** use the `slice_sample` dplyr function to create a training set comprising a sample of rows from the full dataset. Use `training_set_size` to set the proportion of rows to include in the training set.
For this challenge, store your training set in a variable called `training_data`.

```{r}
# Your code here.
# training_data <-
```

Okay, now that you have a training set, we need to create the testing set that we'll use to evaluate our model's performance after training.

**Challenge problem:** Use the `anti_join` function in the dplyr package to create a test set.
For this challenge, store your testing set in a variable called `testing_data`.

```{r}
# Your code here
# testing_data <-
```

## Training a simple decision tree classifier

Next, we'll train a decision tree classifier on the training data (just like we did previously).
Specifically, we'll train a decision tree to predict bike rental level as a function of the `temperature`, `rainfall`, and `hour` attributes.
That is, `rental_level` will be our response variable, and `temperature`, `rainfall`, and `hour` will be our predictor attributes.

```{r}
# Use rpart to train a regression tree using the training data
tree_model <- rpart(
  formula = rental_level ~ temperature + rainfall + hour,
  data = training_data,
  parms = list(split = "information")
)
summary(tree_model)
```

We can look at the regression tree visually:

```{r}
rpart.plot(tree_model)
```

**Challenge problem:** Think about what conditions you expect bike rentals to be high or low.
Then, take a look at the decision tree.
Does the model match your intuition?

### Finding the training error

The summary output for our model already gives a lot of useful information about model's training error.
We can also calculate the training error manually.

```{r}
# Use the predict function to get the model's output for each row of the
# training data
tree_training_predictions <- predict(
  tree_model,
  data = training_data,
  type = "class"
)
```

`tree_training_predictions` now holds all of our model's predictions on our training data.
Next, let's manually figure out how many predictions our model got right:

```{r}
# Get correct predictions by counting how many times the prediction matched the training data
num_correct_predictions_training <- length(
  which(tree_training_predictions == training_data$rental_level)
)
num_correct_predictions_training
```

And how many it got wrong:
```{r}
# Get incorrect predictions by counting how many times the prediction did not match the training data
num_wrong_predictions_training <- length(
  which(tree_training_predictions != training_data$rental_level)
)
num_wrong_predictions_training
```

Finally, here's our accuracy on the training data:
```{r}
num_correct_predictions_training / (num_correct_predictions_training + num_wrong_predictions_training)
```

We can also use the `confusionMatrix` function from the `caret` package to build a fancy confusion matrix for us.

```{r}
confusionMatrix(
  table(training_data$rental_level, tree_training_predictions)
)
```

### Finding the testing error

Remember, it's possible that our model overfit the training data, so even if we see good training accuracy, we can't be sure our model is actually good at predicting bike rentals.
We held back testing data to evaluate how well our model generalizes to data that it has never seen before.
Good accuracy on the testing data will give us some confidence in using our model to make business decisions about bike rentals.

**Challenge problem:** Look at how we calculating training error. Write the code necessary to calculate the testing error for the decision tree you trained.

```{r}
# Your code here
```

**Challenge problem:** Look at the testing error that you calculated.
Is it higher, lower, or about the same as your training error?
Based on your training and testing error do you think that your model overfit?

## Exercises

- We did not use all of the attributes in the bike rental data set to build our models.
  Are there attributes that we didn't use that you think would be useful to include in a predictive model?
- Try building a different model using the predictor attributes that you think would be most useful.
  How does the training/testing errors compare to the first model we built?
- Think about the structure of the dataset. What kinds of issues might run into when we randomly divide the data into a training and testing set?
  For example, are we guaranteed to have a good representation of every hour of the day in both our training and testing data if we split the data randomly?
  What other sampling procedures could we use to perform a training/testing split that might avoid some of these issues?

## References

Dua, D. and Graff, C. (2019). UCI Machine Learning Repository <http://archive.ics.uci.edu/ml>. Irvine, CA: University of California, School of Information and Computer Science.